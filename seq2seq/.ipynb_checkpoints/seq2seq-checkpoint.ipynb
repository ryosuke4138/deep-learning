{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T07:38:33.090235Z",
     "start_time": "2019-03-19T07:38:33.086978Z"
    },
    "lang": "en"
   },
   "source": [
    "## Abstract\n",
    "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever\n",
    "large labeled training sets are available, they cannot be used to map sequences to\n",
    "sequences. In this paper, we present a general end-to-end approach to sequence\n",
    "learning that makes minimal assumptions on the sequence structure. Our method\n",
    "uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\n",
    "to a vector of a fixed dimensionality, and then another deep LSTM to decode the\n",
    "target sequence from the vector. Our main result is that on an English to French\n",
    "translation task from the WMT-14 dataset, the translations produced by the LSTM\n",
    "achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU\n",
    "score was penalized on out-of-vocabulary words. Additionally, the LSTM did not\n",
    "have difficulty on long sentences. For comparison, a phrase-based SMT system\n",
    "achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\n",
    "to rerank the 1000 hypotheses produced by the aforementioned SMT system, its\n",
    "BLEU score increases to 36.5, which is close to the previous state of the art. The\n",
    "LSTM also learned sensible phrase and sentence representations that are sensitive\n",
    "to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but\n",
    "not target sentences) improved the LSTM’s performance markedly, because doing\n",
    "so introduced many short term dependencies between the source and the target\n",
    "sentence which made the optimization problem easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "ja"
   },
   "source": [
    "##要約\n",
    "ディープニューラルネットワーク（DNN）は、困難な学習課題で優れたパフォーマンスを達成した強力なモデルです。 DNNはいつでもうまく機能しますが\n",
    "大きなラベル付きトレーニングセットが利用可能です。それらをシーケンスのマッピングに使用することはできません。\n",
    "シーケンス本稿では、シーケンスに対する一般的なエンドツーエンドのアプローチを紹介します。\n",
    "これは、シーケンス構造に対して最小限の仮定しかしません。私たちの方法\n",
    "入力シーケンスをマッピングするために、多層長期メモリ（LSTM）を使用します。\n",
    "固定次元のベクトルに変換し、次に別のディープLSTMを復号化します。\n",
    "ベクターからの標的配列。私たちの主な結果は、フランス語から英語への翻訳です。\n",
    "WMT-14データセットからの翻訳タスク、LSTMによって作成された翻訳\n",
    "テストセット全体で34.8のBLEUスコアを達成する\n",
    "スコアは語彙外の単語にペナルティを科されました。さらに、LSTMはしませんでした\n",
    "長い文章に困難があります。比較のために、フレーズベースのSMTシステム\n",
    "同じデータセットでBLEUスコア33.3を達成します。 LSTMを使用したとき\n",
    "前述のSMTシステムによって生成された1000の仮説を再ランク付けするために、\n",
    "BLEUスコアは36.5に増加します。これは以前の最新技術に近いものです。の\n",
    "LSTMはまた敏感な賢明な句や文の表現を学びました\n",
    "語順にし、能動的および受動的音声に対して比較的不変である。最後に、我々はすべての原文の単語の順序を逆にすることを発見した。\n",
    "これは、LSTMのパフォーマンスを著しく向上させるためです。\n",
    "そのため、ソースとターゲット間に短期間の依存関係が多数導入されました。\n",
    "最適化問題を容易にした文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNNは大きなラベル付きトレーニングセットがあれば優れたパフォーマンスを出す。\n",
    "LSTM(Long Short-Term Memory)を利用\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "ja"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ja",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
