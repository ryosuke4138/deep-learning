{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "<br><br>\n",
    "\n",
    "#### 論文概要\n",
    "主な成果は、初めて翻訳のタスクにおいて、ニューラルネットワーク（deep LSTM）を使ったSequence-to-Sequenceモデルで、フレーズベース統計翻訳（a phrase-based SMT system, PBSMT）を上回るスコアを出したことである。<br>\n",
    "同時に、入力データの反転（Reverse）でパフォーマンスが上がったことと、LSTMが長い文章も適切に翻訳していたことも報告している。<br>\n",
    "\n",
    "#### Sequence-to-Sequence（Seq2Seq）モデル\n",
    "Sequence-to-Sequence（Seq2Seq）モデルは、系列を入力として系列を出力する機構を持つモデル。\n",
    "入力系列をRNN（Recurrent Neural Networks）でベクトルに変換（=Encoder）し、そのベクトルから別のRNNを用いて系列を生成する（=Decoder）ことから、Encoder-Decoderモデルと呼ばれることもある。単純なRNNでは達成できなかった、言語によって語順や長さが異なる問題に対応することなどが可能になった。<br>\n",
    "\n",
    "\n",
    "Applications（Natural Language processing）\n",
    "<br>using CNN, Attention, Transformer, VAE, GAN, Q-learning, Policy Gradient,or SeqGAN\n",
    "\n",
    "* Translation\n",
    "* Chatbot\n",
    "* Caption generation（画像を入力して画像の説明を生成するタスク）\n",
    "* Reading comprehension\n",
    "* Question answering\n",
    "* Headline generation\n",
    "\n",
    "<br><br>\n",
    "翻訳で使われている Seq2Seqの図[3]\n",
    "<img src=\"./paperPhoto/20171210145927.jpg\">\n",
    "<br>\n",
    "<img src=\"./paperPhoto/seq2seq.jpg\">\n",
    "\n",
    "1. 単語をベクトル表現に変換\n",
    "1. ベクトル表現をRNNに渡す\n",
    "1. `<s>` はdecodeを開始する合図"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    ">Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier\n",
    "\n",
    "* DNN（Deep Neural Networks）は大きなラベル付きトレーニングセットがあれば優れたパフォーマンスを出すが、Seqence-to-sequenceの問題（系列を入力として系列を出力すること）は解決できない。\n",
    "* よって本稿では、時系列データをベクトル表現にする（Encoder）、またはその逆（Decoder）をdeep LSTM（Long Short-Term Memory）で行った、 End to end（入力から出力まで）な Sequence learning を示す。\n",
    "* 主な結果として、英語からフランス語への翻訳のタスクにおいて、評価はBLEU（Bilingual Evaluation Understudy）スコアで行った。\n",
    "(他の評価方法には、人による評価（ネイティブ翻訳者による評価）と自動評価（TERスコア）がある)\n",
    "* 結果、フレーズベース統計翻訳（a phrase-based SMT system, PBSMT）を上回るスコアを出した。\n",
    "* LSTMは語順を敏感に学んだ。また、比較的一定の語順や長さの受動態-能動態の表現を学んだ。\n",
    "* LSTMでは、原文の単語の順序を逆にする事でLSTMのパフォーマンスが向上した。\n",
    "\n",
    ">**統計翻訳（Statistical Machine Translation, SMT）**<br>\n",
    "原文と訳文を大量に集めた対訳データと統計的な学習アルゴリズムだけで翻訳システムを構築する手法。<br>\n",
    "\n",
    ">**BLEU（Bilingual Evaluation Understudy）スコア**<br>\n",
    "$$BLEU = e^{1−r/c} exp(\\sum_{n=1}^N w_n log p_n)$$\n",
    ">>r(Reference) は人間が翻訳した文の長さ<br>\n",
    ">>c(Candidate) は機械が翻訳した文の長さ<br>\n",
    ">>$w_n$は適当な重み<br>\n",
    ">>$p_n$はmodified n-gram precisions<br><br>\n",
    ">**brevity penalty**<br>\n",
    "右辺第一項で、CandidateよりReferenceの方が長いパターンに対してペナルティを課している。（Candidateが短すぎる場合に対してのペナルティ）<br><br>\n",
    "**modified n-gram precisions**<br>\n",
    "precisionを計算するときにReference中の単語は一回使うともう使えないという制約を加えている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T08:17:56.401073Z",
     "start_time": "2019-03-19T08:17:56.391964Z"
    }
   },
   "source": [
    "## Introduction\n",
    "<img src=\"./paperPhoto/Screen Shot 2019-03-19 at 17.25.55.png\">\n",
    "\n",
    "* モデルの入力文が\"ABC\"、出力文が\"WXYZ\"である。\n",
    "* EOS(end-of-sentence)トークンは文末を表す仮想単語である。\n",
    "* LSTMの入力文章の読み込む順番を逆にすることで、多くの短い依存関係を作ることで最適化計算をより早くした。\n",
    "\n",
    ">Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps. A surprising example of the power of DNNs is their ability to sort N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are related to conventional statistical models, they learn an intricate computation. Furthermore, large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the network's parameters. Thus, if there exists a parameter setting of a large DNN that achieves good results (for example, because humans can solve the task very rapidly), supervised backpropagation will find these parameters and solve the problem. Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori. For example, speech recognition and machine translation are sequential problems. Likewise, question answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer. It is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful. Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and fixed. In this paper, we show that a straightforward application of the Long Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems. The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed- dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model [28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs (fig. 1). There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18] who were the first to map the entire input sentence to vector, and is very similar to Choetal. [5]. Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on different parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanauetal. [2]. The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, although it assumes a monotonic alignment between the inputs and the outputs [11].\n",
    "\n",
    "* DNNは音声認識、視覚オブジェクト認識などの困難な複雑な問題に対して優れた性能を発揮する機械学習モデルであり、教師ありの誤差逆伝播で学習することができる。\n",
    "* しかし、DNNは入力と出力が固定次元のベクトルでエンコードできる問題にしか適用できない。\n",
    "* 多くの重要な問題(音声認識や機械翻訳、質問応答)は逐次問題で長さが先験的に知られていないSequenceで最もよく表現されるため、DNNは上手く表現できない。\n",
    "* 本稿では、LSTMが一般的なsequence to sequence learningを解決する事ができる事を示す。\n",
    "* まず、最初のLSTMがtimesteps毎に一旦固定次元のベクトルでsequenceをエンコードして行くことでベクトル表現を得る。次のLSTMがtimesteps毎にベクトルからsequenceをデコードする。\n",
    "* 長い依存関係を学習できることからLSTMを選んだ。\n",
    "* LSTMは長距離の時間依存性を持つデータをうまく学習することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    ">The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural networks to sequences. Given a sequence of inputs (x1 , . . . , xT ), a standard RNN computes a sequence of outputs (y1 , . . . , yT ) by iterating the following equation:\n",
    "$$ h_t = sigm(W^{hx}x_t W^{hh} h_{t-1})$$\n",
    "$$ y_t = W^{yh} h_t $$\n",
    "The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relation- ships.\n",
    "A simple strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN (this approach has also been taken by Choetal. [5]). While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies [14, 4] (figure 1) [16, 15]. However, the Long Short-Term Memory (LSTM) [16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.<br>\n",
    "\n",
    "#### 再帰ニューラルネット(recurrent neural networks; RNN)<br>\n",
    "以下3文は同じ意味。\n",
    "* RNNは可変長の入力列を扱うことに優れたネットワーク構造である。<br>\n",
    "* RNNは自己回帰型の構造をもつニューラルネットワークの総称である。<br>\n",
    "* RNNは時系列データ向けのニューラルネットワーク(NN)である。<br>\n",
    "(a natural generalization of feedforward neural networks to sequences)<br><br>\n",
    "by iterating the following equation<br>\n",
    "$$ h_t = sigm(W^{hx}x_t W^{hh} h_{t-1})$$\n",
    "$$ y_t = W^{yh} h_t $$\n",
    ">inputs 長さTの時系列データ(x1 , . . . , xT )<br>\n",
    "outputs (y1 , . . . , yT )<br>\n",
    "*活性化関数は恒等関数である<br>\n",
    "$W^{xh}∈ℝ|hidden|×|input|$: 入力層から隠れ層への重み<br>\n",
    "$W^{hh}∈ℝ|hidden|×|hidden|$: 隠れ層から隠れ層への重み<br>\n",
    "$W^{yh}∈ℝ|output|×|hidden|$: 隠れ層から出力層への重み<br>\n",
    "*出力層から中間層への期間経路の重みは提示されていない\n",
    "\n",
    "* RNNは長い時系列データではネットワークが時系列長に比例して非常に深くなる。<br>\n",
    "* よって、勾配消失が容易に発生し、情報が上手く伝達されない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    ">The goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT ′ |x1, . . . , xT ) where (x1,...,xT)isaninputsequenceandy1,...,yT′ isitscorrespondingoutputsequencewhoselength T ′ may differ from T . The LSTM computes this conditional probability by first obtaining the fixed- dimensional representation v of the input sequence (x1 , . . . , xT ) given by the last hidden state of the LSTM, and then computing the probability of y1 , . . . , yT ′ with a standard LSTM-LM formulation whose initial hidden state is set to the representation v of x1 , . . . , xT :\n",
    "$$ p(y_1,....,y_{T'}|x_1,....,x_T)=\\prod_{t=1}^{T'}p(y_t|v,y_1,....,y_{t-1}) $$\n",
    "In this equation, each p(yt|v, y1, . . . , yt−1) distribution is represented with a softmax over all the words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that each sentence ends with a special end-of-sentence symbol “<EOS>”, which enables the model to define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure 1, where the shown LSTM computes the representation of “A”, “B”, “C”, “<EOS>” and then uses this representation to compute the probability of “W”, “X”, “Y”, “Z”, “<EOS>”.\n",
    "Our actual models differ from the above description in three important ways. First, we used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found it extremely valuable to reverse the order of the words of the input sentence. So for example, instead of mapping the sentence a,b,c to the sentence α,β,γ, the LSTM is asked to map c,b,a to α,β,γ, where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. We found this simple data transformation to greatly boost the performance of the LSTM.<br>\n",
    "\n",
    "#### ゲート付再帰ニューラルネット<br>\n",
    "RNNは時間方向に深いニューラルネットとなっているため、時間的に離れている時刻に発生した誤差を伝播させるのは勾配消失の影響で困難であった。<br>\n",
    "よって、長期間の依存関係を表現するためのパラメータ（長期記憶（long-term memory））を学習するのが苦手で、直近の依存関係（短期記憶（short-term memory））だけを学習してしまう傾向があった。これをゲートを導入することで長期記憶と短期記憶をバランス良く学習することができるモデル。<br>\n",
    "\n",
    "#### 長短期記憶（long short-term memory; LSTM）<br>\n",
    "* LSTMは最も代表的なゲート付き再帰ニューラルネット。<br>\n",
    "* LSTMはある程度長い時系列データに対しても学習ができるよう考案されたモデル<br>\n",
    "ソース言語の文とターゲット言語での文の間の条件付き確率に注目した式が、\n",
    "$$ p(y_1,....,y_{T'}|x_1,....,x_T)=\\prod_{t=1}^{T'}p(y_t|v,y_1,....,y_{t-1}) $$\n",
    "$y_t$は全てのワードに対して配布される。<br>\n",
    "* 右辺に各確率分布をモデル化して、トレーニングデータを使って学習させることを考えている。<br>\n",
    "* このt-1までの確率分布Pを用いて$y^t$としてどの単語を採用するかを決めている。\n",
    "* この作業を繰り返すことで、入力文に対する翻訳文全体が出来上がる。<br>\n",
    "* Figure 1はA, B, C, を計算し、そしてW, X, Y, Z, それぞれの確率を求めている。<br>\n",
    "\n",
    "\n",
    "#### 本稿で実際にLSTMを使う際の3つの変更点\n",
    "\n",
    "* 入力層と出力層を少し異なる構造を持たせる。2層のLSTMにすることで、計算コストはそこまで増えないのに対し、様々な言語ペアの学習を実現できる。<br>\n",
    "    勾配消失を防ぐため。\n",
    "* 4層のLSTMを使用すること。<br>\n",
    "    深いSLTMの方が性能が良いため。\n",
    "* 入力の語順を反転させること。<br>\n",
    "    a, b, c → α, β, γ より c, b, a → α, β, γ の方が近接関係が統一的になり、性能が向上すると知られているから。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience\n",
    ">We applied our method to the WMT’14 English to French MT task in two ways. We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation.\n",
    "\n",
    "WMT’14英語-フランス語の翻訳タスクを2通り\n",
    "1. SMTを使わず、時系列データを直接翻訳。\n",
    "1. SMTベースのn-best lists(解析した時にスコアを降順にソートしたリスト)を翻訳。\n",
    "\n",
    "の方法でaccuracyを記録、サンプル翻訳を記載し、結果を可視化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset details\n",
    ">We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sen- tences consisting of 348M French words and 304M English words, which is a clean “selected” subset from [29]. We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the baseline SMT [29].\n",
    "As typical neural language models rely on a vector representation for each word, we used a fixed vocabulary for both languages. We used 160,000 of the most frequent words for the source language and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was replaced with a special “UNK” token.\n",
    "\n",
    "* [WMT'14 English to Frenchのデータセット](http://www.statmt.org/wmt14/translation-task.html)\n",
    "* 1200万の文章、3億4800万単語のフランス語と3億400万単語の英語をトレーニングデータとして使用した。\n",
    "* 単語をベクトル表現する為に、ボキャブラリを利用した。\n",
    "* ソース言語(英語)では16万の頻出単語を、そしてターゲット言語(フランス語)では8万の頻出単語を使用した。ボキャブラリに存在しない単語はUNK(unknown)として扱った。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding and Rescoring\n",
    ">The core of our experiments involved training a large deep LSTM on many sentence pairs. We trained it by maximizing the log probability of a correct translation T given the source sentence S, so the training objective is\n",
    "$$ \\frac 1 {|S|} \\sum_{(T,S)∈S} log p(T|S)$$\n",
    "where S is the training set. Once training is complete, we produce translations by finding the most likely translation according to the LSTM:\n",
    "$$ \\hat{T} = arg\\max_{T}p(T|S) $$\n",
    "We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some translation. At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model’s log probability. As soon as the “<EOS>” symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search (Table 1).\n",
    "We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took an even average with their score and the LSTM’s score.\n",
    "    \n",
    "英語とフランス語の文章のペアのデータセットを用いてLSTMを学習させる。\n",
    "$$ \\frac 1 {|S|} \\sum_{(T,S)∈S} log p(T|S)$$\n",
    "S: source(ソース)言語 (英語)<br>\n",
    "T: target(ターゲット)言語 (フランス語)<br>\n",
    "Σの下のSはトレーニングのデータセット<br>\n",
    "<br>\n",
    "トレーニングが終了すると、予測値$\\hat{T}$を見つける。<br>\n",
    "$$ \\hat{T} = arg\\max_{T}p(T|S) $$\n",
    "\n",
    "* left-to-right beam serch decoderを使用すると、少ない候補を維持する事でメモリの消費を抑える事ができ、スコアが良かった。\n",
    "* 1000-bestで整理したLSTMを使用すると、平均スコアもLSTMのスコアも上回った。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing the Source Sentenses\n",
    ">While the LSTM is capable of solving problems with long term dependencies, we discovered that the LSTM learns much better when the source sentences are reversed (the target sentences are not reversed). By doing so, the LSTM’s test perplexity dropped from 5.8 to 4.7, and the test BLEU scores of its decoded translations increased from 25.9 to 30.6.\n",
    "While we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset. Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence. As a result, the problem has a large “minimal time lag” [17]. By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the first few words in the source language are now very close to the first few words in the target language, so the problem’s minimal time lag is greatly reduced. Thus, backpropagation has an easier time “establishing communication” between the source sentence and the target sentence, which in turn results in substantially improved overall performance.\n",
    "Initially, we believed that reversing the input sentences would only lead to more confident predictions in the early parts of the target sentence and to less confident predictions in the later parts. However, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences results in LSTMs with better memory utilization.\n",
    "\n",
    "LSTMはソース言語を逆転させた時に性能が向上した。\n",
    "* ソース言語の語順を反転させること。<br>\n",
    "    a, b, c → α, β, γ より c, b, a → α, β, γ の方が近接関係が統一的になり、性能が向上すると知られているから。\n",
    "* perplexityは5.0から4.7に減少した。BLEUは25.9から30.6に上昇した。\n",
    "* この現象はまだ完全に説明できない。\n",
    "* 同じ意味の単語同士が遠くなる事でラグが発生するが、ソース言語の語順を反転させることで、より等しくした。\n",
    "* よって、誤差逆伝播の時間が減り、全体的なパフォーマンスが上がった。\n",
    "* 始めは、文の後半部分の予測が上手くいかず、パフォーマンスが上がらないと思っていた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training details\n",
    ">We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers, with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary of 160,000 and an output vocabulary of 80,000. We found deep LSTMs to significantly outperform shallow LSTMs, where each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 380M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M for the “decoder” LSTM). The complete training details are given below:\n",
    "• We initialized all of the LSTM’s parameters with the uniform distribution between -0.08 and 0.08\n",
    "• We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7. After 5 epochs, we begun halving the learning rate every half epoch. We trained our models for a total of 7.5 epochs.\n",
    "• We used batches of 128 sequences for the gradient and divided it the size of the batch (namely, 128).\n",
    "• Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\n",
    "exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,25] by scaling it when its norm exceeded a threshold. For each training batch, we compute $s = ∥g∥$ , where g is the gradient divided by 128. If s > 5, we set $g = \\frac {5g} s$\n",
    "• Different sentences have different lengths. Most sentences are short (e.g., length 20-30) but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences, and as a result, much of the computation in the minibatch is wasted. To address this problem, we made sure that all sentences within a minibatch were roughly of the same length, which a 2x speedup.\n",
    "\n",
    "* 4層のLSTM, 層あたり1000個のcell, 1000次元のword embeddingを利用した。\n",
    "* 入力160,000語, 出力80,000語, 結果3億8,000万パラメータができた。\n",
    "* 出力には、ナイーブsoftmaxを利用した。\n",
    "<br>\n",
    "* LSTMのパラメータ初期値は-0.08から0.08の一様分布から取った。\n",
    "* 慣性$＋αΔw^tを$付与しないSGD(確率的勾配降下法)を利用した。学習率は0.7で、5エポック以降半減させ、トータルで7.5エポック学習する。\n",
    "$$ w^{t+1}  \\gets w^t - \\eta \\frac {\\partial {E(w^t)}} {\\partial w^t}$$\n",
    "* バッチ数は128で行った。\n",
    "* LSTMは勾配消失の問題はあまり受けないが、勾配爆発する可能性がある。よって、閾値を定め、以下の式でスケーリングした。\n",
    "$$ s > 5, g = \\frac {5g} s$$\n",
    "* 多くは短い文章(20-30語)を使ったが、長い文章(100語以上)がミニバッチに紛れていた為、多くの計算を無駄にした。よって、文章の長さは揃えた方が良い。(計算が2倍早くなる)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallezation\n",
    "> A C++ implementation of deep LSTM with the configuration from the previous section on a single GPU processes a speed of approximately 1,700 words per second. This was too slow for our purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU (or layer) as soon as they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible for multiplying by a 1000 × 20000 matrix. The resulting implementation achieved a speed of 6,300 (both English and French) words per second with a minibatch size of 128. Training took about a ten days with this implementation.\n",
    "\n",
    "* C++でLSTMを計算すると1,700語/s/GPUで学習した。これは経験則としてはとても遅い。\n",
    "* 8つのGPUを使い、4つのGPUをそれぞれのLSTM層の計算、残りの4つを活性化関数の計算に並列して割り当てた。\n",
    "* 結果、6,300語/s/8GPUとなり、計算に10日間かかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Results\n",
    ">We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our BLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth. This way of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29]. However, if we evaluate the state of the art system of [9] (whose predictions can be downloaded from statmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by statmt.org\\matrix.\n",
    "The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs that differ in their random initializations and in the random order of minibatches. While the decoded translations of the LSTM ensemble do not beat the state of the art, it is the first time that a pure neural translation system outperforms a phrase-based SMT baseline on a large MT task by  sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is within 0.5 BLEU points of the previous state of the art by rescoring the 1000-best list of the baseline system.\n",
    "\n",
    "<img src=\"./paperPhoto/Screen Shot 2019-03-19 at 17.26.31.png\">\n",
    "\n",
    "* 評価手法には、multi-bleu.plというプログラムを用いて計算して求めたBLEUを使った。初期値やミニバッチの順番が違う複数のLSTMのアンサンブルを利用した結果、37.0(state-of-the-art)を記録した。\n",
    "* NNによる翻訳システムがSMTに初めて性能で上回った。\n",
    "> SMT(statistical machine learning): translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual text corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on long sentences\n",
    ">We were surprised to discover that the LSTM did well on long sentences, which is shown quantitatively in figure 3. Table 3 presents several examples of long sentences and their translations.\n",
    "\n",
    "<img src=\"paperPhoto/Screen Shot 2019-03-19 at 17.27.01.png\">\n",
    "<img src=\"paperPhoto/Screen Shot 2019-03-19 at 17.27.13.png\">\n",
    "\n",
    "* LSTMが長い文章に対して驚くほど良い結果を残した。Figure3では量的にグラフで比べ、Table3ではその例を示した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n",
    "<img src=\"paperPhoto/Screen Shot 2019-03-19 at 17.26.51.png\">\n",
    "\n",
    "* Figure2ではLSTMの隠れ層の状態をPCA(主成分分析)で2次元にしたもの。\n",
    "* 能動態-受動態に鈍感であるが、bag-of-wordsモデルでは達成できなかった、語順に対しては敏感に反応している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work\n",
    ">There is a large body of work on applications of neural networks to machine translation. So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM) [23] or a Feedforward Neural Network Language Model (NNLM) [3] to an MT task is by rescoring the n-best lists of a strong MT baseline [22], which reliably improves translation quality.\n",
    "More recently, researchers have begun to look into ways of including information about the source language into the NNLM. Examples of this work include Auli et al. [1], who combine an NNLM with a topic model of the input sentence, which improves rescoring performance. Devlin et al. [8] followed a similar approach, but they incorporated their NNLM into the decoder of an MT system and used the decoder’s alignment information to provide the NNLM with the most useful words in the input sentence. Their approach was highly successful and it achieved large improvements over their baseline.\n",
    "Our work is closely related to Kalchbrenner and Blunsom [18], who were the first to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional neural networks, which lose the ordering of the words. Similarly to this work, Cho et al. [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their primary focus was on integrating their neural network into an SMT system. Bahdanau et al. [2] also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al. [5] and achieved encouraging results. Likewise, Pouget-Abadie et al. [26] attempted to address the memory problem of Cho et al. [5] by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach. We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences.\n",
    "End-to-end training is also the focus of Hermann et al. [12], whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space. However, their approach cannot generate translations directly: to get a translation, they need to do a look up for closest vector in the pre-computed database of sentences, or to rescore a sentence.\n",
    "\n",
    "#### 関連研究\n",
    "* NNを応用したmachine translation(機械翻訳)は多く存在する。\n",
    "* 最もシンプルで効果的なのはRNNLM(RNN-Language Model)または、NNLM(Feedforward Neural Network Language Model)である。\n",
    "* 最近では、研究者はソース言語の情報をNNLMに入れようとしている。\n",
    "* KalchbrennerとBlunsomが近い研究をしていて、初めてベクトルの中に文章情報を付加した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "> In this work, we showed that a large deep LSTM with a limited vocabulary can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task. The success of our simple LSTM-based approach on MT suggests that it should do well on many other sequence learning problems, provided they have enough training data.\n",
    "We were surprised by the extent of the improvement obtained by reversing the words in the source sentences. We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler. In particular, while we were unable to train a standard RNN on the non-reversed translation problem (shown in fig. 1), we believe that a standard RNN should be easily trainable when the source sentences are reversed (although we did not verify it experimentally).\n",
    "We were also surprised by the ability of the LSTM to correctly translate very long sentences. We were initially convinced that the LSTM would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours [5, 2, 26]. And yet, LSTMs trained on the reversed dataset had little difficulty translating long sentences.\n",
    "Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized approach can outperform a mature SMT system, so further work will likely lead to even greater translation accuracies. These results suggest that our approach will likely do well on other challenging sequence to sequence problems.\n",
    "\n",
    "* 本稿では、LSTMがこれまでのSTMに性能を上回ったことを示した。我々は、Sequence to sequenceのタスクに対して良いアプローチであったと言える。この方法を、強化することでされに翻訳の精度を上げられるだろう。また、データが十分にあれば、他の時系列データのタスクにも応用できるであろう。\n",
    "* 語順を逆にすることでパフォーマンスが上がったことには驚いた。これは、エンコードの工夫でタスクをシンプルにすることができるということである。\n",
    "* LSTMが長い文章も適切に翻訳していたことにも驚いた。始めは、メモリ不足で失敗すると考えていた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. Joint language and translation modeling with recurrent\n",
    "neural networks. In EMNLP, 2013.<br>\n",
    "[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.\n",
    "arXiv preprint arXiv:1409.0473, 2014.<br>\n",
    "[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In Journal of\n",
    "Machine Learning Research, pages 1137–1155, 2003.<br>\n",
    "[4] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.<br>\n",
    "IEEE Transactions on Neural Networks, 5(2):157–166, 1994.<br>\n",
    "[5] K. Cho, B. Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Arxiv preprint arXiv:1406.1078,\n",
    "2014.<br>\n",
    "[6] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.<br>\n",
    "In CVPR, 2012.<br>\n",
    "[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\n",
    "vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing - Special\n",
    "Issue on Deep Learning for Speech and Language Processing, 2012.<br>\n",
    "[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. Fast and robust neural network\n",
    "joint models for statistical machine translation. In ACL, 2014.<br>\n",
    "[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based machine\n",
    "translation systems for wmt-14. In WMT, 2014.<br>\n",
    "[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,\n",
    "2013.<br>\n",
    "[11] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal class ´ ification: labelling\n",
    "unsegmented sequence data with recurrent neural networks. In ICML, 2006.<br>\n",
    "[12] K. M. Hermann and P. Blunsom. Multilingual distributed representations without word alignment. In\n",
    "ICLR, 2014.<br>\n",
    "[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,\n",
    "T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\n",
    "Signal Processing Magazine, 2012.<br>\n",
    "[14] S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Master’s thesis, Institut fur Informatik, Technische Universitat, Munchen, 1991.<br>\n",
    "[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficulty\n",
    "of learning long-term dependencies, 2001.<br>\n",
    "[16]  [S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) <br>\n",
    "[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997.<br>\n",
    "[18] N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, 2013.<br>\n",
    "[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural\n",
    "networks. In NIPS, 2012.<br>\n",
    "[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building\n",
    "high-level features using large scale unsupervised learning. In ICML, 2012.<br>\n",
    "[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\n",
    "Proceedings of the IEEE, 1998.<br>\n",
    "[22] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of\n",
    "Technology, 2012.<br>\n",
    "[23] T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur. Recurrent neural network based \n",
    "language model. In INTERSPEECH, pages 1045–1048, 2010.<br>\n",
    "[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a method for automatic evaluation of machine\n",
    "translation. In ACL, 2002.<br>\n",
    "[25] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. arXiv\n",
    "preprint arXiv:1211.5063, 2012.<br>\n",
    "[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. Overcoming the\n",
    "curse of sentence length for neural machine translation using automatic segmentation. arXiv preprint\n",
    "arXiv:1409.1257, 2014.<br>\n",
    "[27] A. Razborov. On small depth threshold circuits. In Proc. 3rd Scandinavian Workshop on Algorithm\n",
    "Theory, 1992.<br>\n",
    "[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.\n",
    "Nature, 323(6088):533–536, 1986.<br>\n",
    "[29]  [H. Schwenk. University le mans.](http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/) , 2014. [Online; accessed 03-September-2014].<br>\n",
    "[30] M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTERSPEECH, 2010.<br>\n",
    "[31] P. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of IEEE, 1990.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T11:39:31.651271Z",
     "start_time": "2019-03-20T11:39:31.621740Z"
    }
   },
   "source": [
    "## 参考文献\n",
    "1. [Sequence to Sequence Learning\n",
    "with Neural Networks(原文)](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "\n",
    "1. [統計翻訳に構造制約を導入する新しいアプローチ](http://www.japio.or.jp/00yearbook/files/2008book/08_5_05.pdf)\n",
    "\n",
    "1. [ 自動評価尺度BLEU ](http://www2.nict.go.jp/astrec-att/member/mutiyama/corpmt/4.pdf)\n",
    "\n",
    "1. [Translation with a Sequence to Sequence Network and Attention](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb)\n",
    "\n",
    "1. [Neural Machine Translation (seq2seq) Tutorial](https://github.com/tensorflow/nmt/blob/master/README.md)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "ja"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ja",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
